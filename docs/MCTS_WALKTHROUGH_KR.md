# MCTS 수치 예시 워크스루

이 문서는 GPU Advisor의 MCTS(몬테카를로 트리 탐색)가 실제로 어떻게 동작하는지 **숫자로** 보여줍니다.

## 전체 흐름

```
루트 상태 (RTX 4090의 256D 잠재 벡터)
    │
    ├── [시뮬레이션 1] 선택 → 확장 → 롤아웃 → 역전파
    ├── [시뮬레이션 2] 선택 → 확장 → 롤아웃 → 역전파
    ├── ...
    └── [시뮬레이션 50] 선택 → 역전파
    │
    └── 방문 횟수 기반 행동 확률 계산
```

## 초기 상태

RTX 4090에 대해 MCTS를 실행한다고 가정합니다.

```
루트 노드:
  상태: s_0 = [0.073, 0.068, 0.065, ...] (256차원 잠재 벡터)
  방문 횟수: N = 0
  가치 합계: W = 0
  자식 노드: 없음 (미확장)
```

## 시뮬레이션 1: 첫 번째 확장

### 1단계: 선택 (Selection)

루트가 미확장이므로, 바로 확장으로 진행합니다.

### 2단계: 확장 (Expansion)

Prediction Network f(s_0)가 정책과 가치를 출력합니다:

```
f(s_0) → 정책 로짓: [-0.23, 0.45, 0.12, -0.08, -0.31]
         → softmax → P = [0.128, 0.253, 0.182, 0.149, 0.118]
                         BUY   WAIT_S WAIT_L HOLD   SKIP

         가치: v = 0.15
```

5개 자식 노드가 생성됩니다:

```
루트 (N=0, W=0)
├── [BUY_NOW]    사전확률 P=0.158, N=0, W=0
├── [WAIT_SHORT] 사전확률 P=0.311, N=0, W=0  ← 최대 사전확률
├── [WAIT_LONG]  사전확률 P=0.224, N=0, W=0
├── [HOLD]       사전확률 P=0.183, N=0, W=0
└── [SKIP]       사전확률 P=0.125, N=0, W=0
```

### 3단계: 시뮬레이션 (Rollout)

WAIT_SHORT 자식(최대 사전확률)에서 5단계 롤아웃:

```
단계 1: 상태 s_0 → f(s_0) → 행동 선택(확률적) → BUY_NOW
        g(s_0, BUY_NOW) → s_1, 보상 r_1 = +0.012

단계 2: 상태 s_1 → f(s_1) → 행동 선택 → HOLD
        g(s_1, HOLD) → s_2, 보상 r_2 = -0.003

단계 3: 상태 s_2 → f(s_2) → 행동 선택 → WAIT_SHORT
        g(s_2, WAIT_SHORT) → s_3, 보상 r_3 = +0.008

단계 4: 상태 s_3 → f(s_3) → 행동 선택 → BUY_NOW
        g(s_3, BUY_NOW) → s_4, 보상 r_4 = +0.005

단계 5: 상태 s_4 → f(s_4) → 행동 선택 → HOLD
        g(s_4, HOLD) → s_5, 보상 r_5 = -0.001

최종 가치: f(s_5) → v_5 = 0.12
```

총 보상 계산 (할인율 γ = 0.99):

```
V = r_1 + γ×r_2 + γ²×r_3 + γ³×r_4 + γ⁴×r_5 + γ⁵×v_5
  = 0.012 + 0.99×(-0.003) + 0.99²×0.008 + 0.99³×0.005 + 0.99⁴×(-0.001) + 0.99⁵×0.12
  = 0.012 - 0.00297 + 0.00784 + 0.00490 - 0.00096 + 0.11416
  = 0.1350
```

### 4단계: 역전파 (Backup)

값 0.1350을 경로를 따라 역전파:

```
WAIT_SHORT 노드: N = 0→1, W = 0→0.1350, Q = 0.1350
루트 노드:       N = 0→1, W = 0→0.1350, Q = 0.1350
```

## 시뮬레이션 2: UCB 기반 선택

### 1단계: 선택 — UCB 점수 계산

```
UCB(a) = Q(a) + c × P(a) × √(ln(N_parent + 1) / (1 + N_child))

루트 방문 횟수 N_parent = 1, c = √2 ≈ 1.414

BUY_NOW:    Q=0    + 1.414 × 0.158 × √(ln(2)/1) = 0 + 0.186 = 0.186
WAIT_SHORT: Q=0.135 + 1.414 × 0.311 × √(ln(2)/2) = 0.135 + 0.259 = 0.394 ← 이미 방문
WAIT_LONG:  Q=0    + 1.414 × 0.224 × √(ln(2)/1) = 0 + 0.264 = 0.264
HOLD:       Q=0    + 1.414 × 0.183 × √(ln(2)/1) = 0 + 0.215 = 0.215
SKIP:       Q=0    + 1.414 × 0.125 × √(ln(2)/1) = 0 + 0.147 = 0.147

* 미방문 노드(N=0)는 UCB = ∞이므로 우선 탐색
→ BUY_NOW 선택 (미방문 중 하나)
```

> **핵심 포인트**: 미방문 노드의 UCB = ∞이므로, 처음 5회 시뮬레이션은 각 행동을 한 번씩 탐색합니다.

### 이후 롤아웃 & 역전파

BUY_NOW에서 5단계 롤아웃 → 총 보상 0.0820

```
BUY_NOW 노드:    N = 0→1, W = 0→0.082, Q = 0.082
루트 노드:       N = 1→2, W = 0.135→0.217, Q = 0.108
```

## 시뮬레이션 3~5: 나머지 행동 탐색

```
시뮬레이션 3: WAIT_LONG  → 롤아웃 → V = 0.1120 → 역전파
시뮬레이션 4: HOLD       → 롤아웃 → V = 0.0450 → 역전파
시뮬레이션 5: SKIP       → 롤아웃 → V = 0.0280 → 역전파
```

5회 시뮬레이션 후 트리 상태:

```
루트 (N=5, Q=0.080)
├── [BUY_NOW]    N=1, Q=0.082
├── [WAIT_SHORT] N=1, Q=0.135  ← 현재 최고
├── [WAIT_LONG]  N=1, Q=0.112
├── [HOLD]       N=1, Q=0.045
└── [SKIP]       N=1, Q=0.028
```

## 시뮬레이션 6: 본격적 UCB 기반 탐색

```
UCB 점수 (N_parent = 5):

BUY_NOW:    0.082 + 1.414 × 0.158 × √(ln(6)/2) = 0.082 + 0.237 = 0.319
WAIT_SHORT: 0.135 + 1.414 × 0.311 × √(ln(6)/2) = 0.135 + 0.467 = 0.602 ← 최대
WAIT_LONG:  0.112 + 1.414 × 0.224 × √(ln(6)/2) = 0.112 + 0.337 = 0.449
HOLD:       0.045 + 1.414 × 0.183 × √(ln(6)/2) = 0.045 + 0.275 = 0.320
SKIP:       0.028 + 1.414 × 0.125 × √(ln(6)/2) = 0.028 + 0.188 = 0.216

→ WAIT_SHORT 선택 (Q 값 + 사전확률 모두 높음)
```

## 시뮬레이션 50 후: 최종 트리

50회 시뮬레이션 후 예시:

```
루트 (N=50, Q=0.095)
├── [BUY_NOW]    N=8,  Q=0.078  → 확률 16%
├── [WAIT_SHORT] N=18, Q=0.142  → 확률 36%  ← 최다 방문
├── [WAIT_LONG]  N=12, Q=0.118  → 확률 24%
├── [HOLD]       N=7,  Q=0.052  → 확률 14%
└── [SKIP]       N=5,  Q=0.031  → 확률 10%
```

## 행동 확률 계산

방문 횟수를 온도(τ)로 변환:

```
π(a) = N(a)^(1/τ) / Σ N(a')^(1/τ)

τ = 1.0 (기본값):
π = [8, 18, 12, 7, 5] / 50 = [0.16, 0.36, 0.24, 0.14, 0.10]

τ = 0 (탐욕적):
π = [0, 1, 0, 0, 0]  (WAIT_SHORT만 선택)
```

## 정책 보정 후 최종 결정

MCTS 정책은 다른 신호들과 혼합됩니다:

```
MCTS 정책:   [0.16, 0.36, 0.24, 0.14, 0.10]
보상 정책:   [0.15, 0.30, 0.25, 0.18, 0.12]
사전 확률:   [0.20, 0.25, 0.22, 0.18, 0.15]
효용 편향:   [0.10, 0.28, 0.30, 0.17, 0.15]

보정 결과 = 0.45×MCTS + 0.25×보상 + 0.15×사전 + 0.15×효용
         = [0.148, 0.318, 0.249, 0.157, 0.114]

→ 최종 선택: WAIT_SHORT (31.8%)
→ 신뢰도: 0.318
→ 엔트로피: 1.52
```

## 안전장치 확인

```
신뢰도 0.318 ≥ 0.25 (최소 신뢰도) → 통과 ✓
엔트로피 1.52 ≤ 1.58 (최대 엔트로피) → 통과 ✓

→ 안전 모드 비활성화
→ 최종 행동: WAIT_SHORT (원본 유지)
```

## 핵심 학습 포인트

1. **UCB는 탐험과 활용의 균형**입니다: Q값(활용)과 방문 횟수 기반 보너스(탐험)의 합
2. **미방문 노드는 UCB = ∞**: 처음 5회는 모든 행동을 한 번씩 시도
3. **방문 횟수 = 행동 확률**: 많이 방문할수록 좋은 행동이라는 직관
4. **롤아웃 깊이 5**: Dynamics Network로 5일간의 미래를 시뮬레이션
5. **정책 보정**: MCTS만으로는 한 행동에 쏠릴 수 있어 다른 신호로 보정

---

**관련 코드**: `backend/models/mcts_engine.py`의 `MCTSEngine.search()` 메서드
