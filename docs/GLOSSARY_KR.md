# 용어 사전 (Glossary)

GPU Advisor 프로젝트에서 사용되는 AI/ML 용어를 한국어로 설명합니다.

## A

### Action Space (행동 공간)
에이전트가 선택할 수 있는 모든 행동의 집합. 이 프로젝트에서는 {BUY_NOW, WAIT_SHORT, WAIT_LONG, HOLD, SKIP}의 5개 이산 행동.

### AdamW
가중치 감쇠(weight decay)를 분리한 Adam 옵티마이저 변형. 학습률 조정과 정규화를 독립적으로 수행하여 일반화 성능을 개선.

### Anti-Collapse Regularizer (반붕괴 정규화)
정책이 한 행동에 과도하게 집중(모드 붕괴)되는 것을 방지하는 메커니즘. 엔트로피가 임계값 이하로 떨어지면 사전확률 쪽으로 부드럽게 이동.

## B

### Backup (역전파, MCTS)
MCTS에서 시뮬레이션 결과를 리프 노드부터 루트까지 경로를 따라 전파하는 과정. 각 노드의 방문 횟수(N)와 가치 합계(W)를 업데이트.

### Backtest (백테스트)
과거 데이터를 사용하여 에이전트의 의사결정을 검증하는 방법. t일의 결정이 t+1일의 실제 가격 변동에 대해 올바른지 평가.

### Batch Size (배치 크기)
한 학습 스텝에서 사용하는 샘플 수. 이 프로젝트에서는 32.

## C

### Calibration (보정)
MCTS 정책을 다른 신호(보상, 사전확률, 효용)와 혼합하여 최종 행동 확률을 조정하는 과정.

### Checkpoint (체크포인트)
학습된 모델의 가중치를 저장한 파일. `.pth` 확장자. 3개 네트워크(h, g, f)의 state_dict와 메타데이터 포함.

### Confidence (신뢰도)
보정된 정책에서 가장 높은 행동의 확률. 0.0~1.0 범위. 이 프로젝트에서 0.25 미만이면 안전 모드 발동.

### Cross-Entropy Loss (교차 엔트로피 손실)
분류 문제의 표준 손실 함수. 예측된 확률 분포와 실제 레이블 간의 차이를 측정. 정책 학습에 사용.

## D

### Dirichlet Noise (디리클레 노이즈)
MCTS 루트 노드의 사전확률에 추가하는 탐험 노이즈. α = 0.03으로 소량의 무작위성을 부여하여 다양한 행동 탐색을 장려.

### Discount Factor (할인율, γ)
미래 보상의 현재 가치 비율. γ = 0.99이면 1단계 후 보상이 현재의 99% 가치. 장기 계획에 영향.

### Dynamics Network (동역학 네트워크, g)
세계 모델의 일부. 현재 상태 s_t와 행동 a_t가 주어지면 다음 상태 s_{t+1}과 보상을 예측하는 신경망.

## E

### Entropy (엔트로피)
확률 분포의 불확실성 척도. H = -Σ p(x) × log(p(x)). 높은 엔트로피 = 균등한 분포 = 불확실. 낮은 엔트로피 = 집중된 분포 = 확신.

### Expansion (확장, MCTS)
MCTS에서 리프 노드에 자식 노드를 추가하는 단계. Prediction Network가 사전확률을 제공.

## F

### Feature Engineering (특징 공학)
원시 데이터를 모델이 학습하기 좋은 형태로 변환하는 과정. 이 프로젝트에서 11D 원시 → 256D 특징 벡터.

### Fine-Tuning (미세 조정)
사전 학습된 모델을 새로운 데이터로 추가 학습하는 과정. 수집된 일일 데이터로 세계 모델을 지속적으로 개선.

## G

### GELU (Gaussian Error Linear Unit)
활성화 함수의 하나. ReLU의 부드러운 변형으로, 음수 입력도 약간의 기울기를 가짐. Transformer에서 많이 사용.

### Gradient Clipping (기울기 클리핑)
기울기 노름이 임계값(1.0)을 초과하면 스케일링하여 학습 안정성을 보장하는 기법.

## H

### HOLD (관망)
5개 행동 중 하나. 매수도 회피도 하지 않고 상황을 지켜보는 행동. 안전 모드에서 기본 행동으로 사용.

## K

### KL Divergence (쿨백-라이블러 발산)
두 확률 분포 간의 차이를 측정하는 척도. 학습 중 정책이 사전확률 분포에서 너무 벗어나지 않도록 정규화에 사용.

## L

### Latent State (잠재 상태)
Representation Network가 생성하는 256차원 벡터. 원시 시장 데이터의 압축된 표현. 다른 네트워크의 공통 입력.

### LayerNorm (레이어 정규화)
각 샘플 내에서 특징을 정규화하는 기법. 배치 크기에 독립적이므로 작은 배치에서도 안정적.

### Learning Rate (학습률)
옵티마이저가 가중치를 업데이트하는 크기. 이 프로젝트에서 1e-4. 너무 크면 발산, 너무 작으면 수렴이 느림.

## M

### MCTS (Monte Carlo Tree Search, 몬테카를로 트리 탐색)
의사결정을 위한 탐색 알고리즘. 선택 → 확장 → 시뮬레이션 → 역전파의 4단계를 반복하여 최적 행동을 찾음. AlphaGo의 핵심 알고리즘.

### Mode Collapse (모드 붕괴)
모델이 다양한 출력 대신 한 가지 출력에 과도하게 집중하는 현상. 정책이 한 행동에 95% 이상 집중되면 모드 붕괴로 판정.

### Moving Average (이동평균, MA)
최근 N일간의 평균 가격. MA7 = 7일 이동평균, MA14 = 14일 이동평균. 가격 추세 파악에 사용.

### MSE Loss (평균 제곱 오차)
예측값과 실제값의 차이를 제곱한 평균. 가치 예측과 보상 예측 학습에 사용.

### MuZero
DeepMind의 알고리즘. AlphaZero의 확장으로, 환경의 규칙을 몰라도 학습된 세계 모델(Dynamics Network)을 사용하여 계획을 수립.

## N

### Node (노드, MCTS)
MCTS 트리의 각 위치. 상태, 방문 횟수(N), 가치 합계(W), 사전확률(P), 자식 노드 목록을 포함.

## O

### One-Hot Encoding (원핫 인코딩)
범주형 값을 이진 벡터로 변환. 행동 2(WAIT_LONG)는 [0, 0, 1, 0, 0]으로 표현.

## P

### Policy (정책, π)
상태에서 행동으로의 매핑. π(a|s) = 상태 s에서 행동 a를 선택할 확률. Prediction Network의 출력.

### Positional Encoding (위치 인코딩)
시퀀스에서 위치 정보를 벡터에 추가하는 기법. sin/cos 함수를 사용하여 각 위치에 고유한 패턴 부여.

### Prediction Network (예측 네트워크, f)
잠재 상태에서 정책(행동 확률)과 가치를 출력하는 신경망. AlphaGo의 Policy-Value Network에 해당.

### Prior Probability (사전 확률)
학습 데이터의 행동 분포. 실제 데이터에서 각 행동이 얼마나 자주 최적이었는지를 반영.

## Q

### Q-Value (행동-가치, Q)
특정 상태에서 특정 행동의 기대 가치. Q = W/N (총 가치 / 방문 횟수).

### Quality Gates (품질 게이트)
에이전트 배포 전 통과해야 하는 성능 기준. 정확도, 보상, 관망 비율, 엔트로피 등 7가지 게이트.

## R

### Representation Network (표현 네트워크, h)
시장 상태(22D)를 잠재 상태(256D)로 인코딩하는 신경망. 세계 모델의 입구.

### Reward (보상)
행동의 결과에 대한 수치적 피드백. BUY_NOW의 보상 = 실제 가격 변동률. 양수면 좋은 결정, 음수면 나쁜 결정.

### Rollout (롤아웃)
MCTS에서 현재 노드부터 여러 단계 앞을 시뮬레이션하는 과정. Dynamics Network를 사용하여 미래 상태를 예측.

### RSI (Relative Strength Index, 상대강도지수)
가격의 상승/하락 강도를 0~100으로 표현하는 기술 지표. 70 이상 = 과매수, 30 이하 = 과매도.

## S

### Safe Mode (안전 모드)
에이전트의 신뢰도가 낮거나 엔트로피가 높을 때 발동. 모든 행동을 HOLD로 강제 전환.

### Selection (선택, MCTS)
MCTS에서 루트부터 리프까지 UCB 점수가 가장 높은 자식을 따라 내려가는 단계.

### Softmax
로짓(실수)을 확률 분포(0~1, 합계 1)로 변환하는 함수. softmax(x_i) = exp(x_i) / Σ exp(x_j).

### State Vector (상태 벡터)
특정 시점의 시장 상황을 수치로 표현한 벡터. Feature Engineer가 256차원으로 생성.

## T

### Tanh (하이퍼볼릭 탄젠트)
출력을 [-1, 1] 범위로 제한하는 활성화 함수. Value head에서 가치를 정규화된 범위로 출력하는 데 사용.

### Temperature (온도, τ)
MCTS 방문 횟수를 행동 확률로 변환할 때의 탐험 수준. τ=1: 방문 비례, τ=0: 최다 방문 행동만 선택(탐욕적).

### Transition Sample (전이 샘플)
(상태, 다음 상태, 행동, 보상, 가치 목표)의 5튜플. 세계 모델 학습의 기본 데이터 단위.

## U

### UCB (Upper Confidence Bound, 상한 신뢰 한계)
탐험-활용 균형을 위한 점수. UCB = Q(활용) + 탐험 보너스. 방문이 적은 행동에 높은 보너스를 부여하여 탐험을 장려.

### Uplift (향상도)
에이전트의 성능이 기준선(항상 매수, 항상 대기 등) 대비 얼마나 나은지를 측정하는 지표.

## V

### Value (가치, v)
상태의 장기적 기대 수익. v(s) = 상태 s에서 시작하여 최적 정책을 따를 때의 기대 보상 합계. [-1, 1] 범위.

## W

### Weight Decay (가중치 감쇠)
학습 시 가중치 크기에 비례하는 페널티를 추가하여 과적합을 방지하는 정규화 기법. L2 정규화의 다른 이름.

### World Model (세계 모델)
환경의 동작을 시뮬레이션하는 학습된 모델. 이 프로젝트에서 Representation(h) + Dynamics(g) + Prediction(f) 세 네트워크로 구성.

---

**참고**: 이 용어 사전은 프로젝트 코드와 문서에서 사용되는 용어를 기준으로 작성되었습니다.
