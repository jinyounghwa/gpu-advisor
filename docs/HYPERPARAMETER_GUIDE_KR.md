# 하이퍼파라미터 설계 근거

GPU Advisor 프로젝트에서 사용된 모든 핵심 하이퍼파라미터와 그 선택 이유를 설명합니다.

## 1. 잠재 공간 차원: 256D

| 항목 | 값 |
|------|-----|
| 입력 차원 | 22D (시장 상태) |
| 잠재 공간 | 256D |
| 확장 비율 | 약 11.6배 |

**왜 256인가?**

- **정보 용량**: 256D는 6개 카테고리 특징(가격 60D, 환율 20D, 뉴스 30D, 시장 20D, 시간 20D, 기술지표 106D)을 하나의 벡터로 압축하기에 충분한 차원입니다.
- **2의 거듭제곱**: GPU 텐서 연산에서 2^8 = 256은 메모리 정렬과 SIMD 연산에 최적화됩니다.
- **MuZero 논문 참고**: 원본 MuZero는 바둑 19×19 보드(361개 교차점)에 대해 256채널을 사용했습니다. 우리 입력(22D)은 훨씬 작으므로 256D면 충분합니다.
- **128 vs 256 vs 512 비교**:
  - 128D: 6개 특징 카테고리를 구분하기에 병목이 될 수 있음
  - 256D: 카테고리당 ~42D 할당 가능, 적절한 표현력
  - 512D: 파라미터가 2배로 증가하지만, 22D 입력 대비 과적합 위험

## 2. MCTS 시뮬레이션 횟수: 50회

| 항목 | 값 |
|------|-----|
| 시뮬레이션 수 | 50 |
| 행동 수 | 5 |
| 행동당 평균 탐색 | 10회 |

**왜 50인가?**

- **행동 공간 대비**: AlphaGo는 362개 행동에 1,600회 시뮬레이션을 사용합니다 (행동당 ~4.4회). 우리는 5개 행동이므로, 50회면 행동당 10회로 **AlphaGo보다 풍부한 탐색**입니다.
- **추론 속도**: 50회 × 5단계 롤아웃 = 250회 신경망 호출. CPU에서 약 0.5초로, 실시간 API 응답에 적합합니다.
- **수확체감**: 시뮬레이션 횟수가 증가하면 정확도가 올라가지만, 5개 행동에서는 30회 이상부터 수렴하기 시작합니다.

```
시뮬레이션 수 vs 추정 성능:
10회  → 행동당 2회, 탐색 불충분
30회  → 행동당 6회, 기본적 탐색
50회  → 행동당 10회, 안정적 탐색 ← 선택
100회 → 행동당 20회, 추론 시간 2배, 개선 미미
```

## 3. 행동 공간: 5개 행동

| 행동 | 의미 | 설계 근거 |
|------|------|-----------|
| BUY_NOW | 즉시 구매 | 가격 하락 감지 시 매수 |
| WAIT_SHORT | 단기 대기 (1주) | 소폭 하락 예상 |
| WAIT_LONG | 중기 대기 (1개월) | 큰 하락 예상 |
| HOLD | 관망 | 불확실성 높을 때 |
| SKIP | 이번 사이클 패스 | 구매 자체 회피 |

**왜 5개인가?**

- **3개 (매수/대기/관망)**: 너무 단순. "얼마나 대기할지"를 표현하지 못함
- **5개**: 시간 축(즉시/단기/중기)과 강도 축(매수/대기/회피)을 모두 커버
- **7개 이상**: GPU 구매 결정에서 더 세분화할 실질적 차이가 없음
- **알파고 비유**: 바둑에서 한 수는 "이 위치에 돌을 놓다"인데, GPU 구매에서 한 수는 "이 시점에 행동하다"입니다. 시간 축의 자연스러운 구분이 5단계입니다.

## 4. 탐험 상수 (UCB): c = √2 ≈ 1.414

```
UCB(s, a) = Q(s, a) + c × P(s, a) × √(ln(N_parent) / (1 + N_child))
```

**왜 √2인가?**

- **UCB1 이론**: Auer et al. (2002)이 증명한 최적 탐험 상수가 √2입니다.
- **탐험-활용 균형**: c가 작으면 활용(exploitation) 편향, c가 크면 탐험(exploration) 편향
  - c = 1.0: 탐험 부족, 초기 좋은 행동에 고착
  - c = √2: 이론적 최적, 모든 행동을 충분히 탐색
  - c = 2.0: 과도한 탐험, 수렴 느림
- **AlphaGo**: PUCT 변형을 사용하며 c_puct = 1.5~2.5 범위. 우리 값은 이 범위 내입니다.

## 5. 롤아웃 깊이: 5단계

| 항목 | 값 |
|------|-----|
| 롤아웃 깊이 | 5 |
| 할인율 | 0.99 |
| 5단계 후 유효 할인 | 0.99^5 ≈ 0.951 |

**왜 5단계인가?**

- **시간 해석**: 각 단계 = 1일. 5단계 = 향후 5일 시뮬레이션.
- **GPU 가격 특성**: 가격 변동의 주요 패턴은 주간(5영업일) 단위로 나타남
- **오차 누적**: Dynamics Network의 예측 오차는 단계마다 누적됩니다. 5단계 이후 예측 신뢰도가 급격히 감소
- **계산 비용**: 롤아웃 깊이 × 시뮬레이션 수 = 총 신경망 호출. 5 × 50 = 250회가 실시간 추론에 적합

## 6. 신경망 아키텍처

### 은닉 차원: 512D

```
입력 256D → 은닉 512D (×4 블록) → 출력 256D/5D/1D
```

- **2배 확장**: 입력 256D의 2배인 512D는 충분한 비선형 표현력을 제공
- **4배 FFN**: 각 블록 내부에서 512 → 2048 → 512으로 4배 확장 (Transformer 관례)
- **파라미터 예산**: 총 18.9M 파라미터를 3개 네트워크에 균등 분배 (~6M each)

### 레이어 수: 4 블록

- **깊이 vs 폭 트레이드오프**: 4개 블록 × 512D는 2개 블록 × 1024D보다 표현력이 높음
- **기울기 흐름**: 4블록은 LayerNorm + 잔차연결 없이도 안정적으로 학습 가능한 깊이
- **MuZero 참고**: 원본은 ResNet 기반 16블록이지만, 우리 입력(22D)은 훨씬 단순하므로 4블록이 적절

## 7. 학습 하이퍼파라미터

| 항목 | 값 | 근거 |
|------|-----|------|
| 학습률 | 1e-4 | AdamW의 안정적 수렴 범위 |
| 배치 크기 | 32 | 일일 24개 GPU 모델 × 데이터 일수에 대한 효율적 샘플링 |
| 학습 스텝 | 500 | 데이터 크기 대비 과적합 방지 |
| 가중치 감쇠 | 1e-5 | L2 정규화로 과적합 억제 |
| 기울기 클리핑 | 1.0 | 학습 안정성 보장 |
| 엔트로피 계수 | 0.001 | 정책 다양성 유지하되 학습 방해하지 않는 수준 |
| 사전확률 정규화 | 0.02 | 정책이 데이터 분포에서 너무 벗어나지 않도록 |

### 학습률 1e-4 근거

```
1e-3: 학습 불안정, 손실 진동
1e-4: 안정적 수렴, 500 스텝 내 수렴 ← 선택
1e-5: 수렴 너무 느림, 5000+ 스텝 필요
```

### 배치 크기 32 근거

- 전체 데이터: ~24개 GPU × 일수 (현재 ~168개 전이 샘플)
- 배치 32는 전체의 ~19%를 매 스텝마다 샘플링
- 작은 데이터셋에서 큰 배치는 과적합, 작은 배치는 노이즈 과다

## 8. 정책 보정 가중치

```python
calibrated = 0.45 × MCTS정책 + 0.25 × 보상정책 + 0.15 × 사전확률 + 0.15 × 효용편향
```

| 출처 | 가중치 | 역할 |
|------|--------|------|
| MCTS 정책 | 0.45 | 핵심 의사결정 (계획 기반) |
| 보상 정책 | 0.25 | 기대 보상 기반 보정 |
| 사전 확률 | 0.15 | 데이터 분포 기반 정규화 |
| 효용 편향 | 0.15 | 관찰 가능한 특징 기반 상식 |

**왜 MCTS가 0.45인가?**

- MCTS 단독(1.0)은 모드 붕괴 위험이 있음 (한 행동에 100% 집중)
- 0.45로 주도권을 유지하면서 다른 신호로 보정
- 합계가 1.0이 되어 확률 분포를 보장

## 9. 안전장치 임계값

| 임계값 | 값 | 근거 |
|--------|-----|------|
| 최소 신뢰도 | 0.25 | 5개 행동의 균등 확률(0.20)보다 높아야 의미 있는 선택 |
| 최대 엔트로피 | 1.58 | 5개 행동의 최대 엔트로피 ln(5) ≈ 1.609의 98% |
| 최소 행동 엔트로피 | 0.25 | 모드 붕괴 방지 (0이면 한 행동만 선택) |
| 최대 관망 비율 | 0.85 | 85% 이상 관망이면 에이전트가 실질적으로 작동하지 않는 것 |
| 최소 정확도 | 0.55 | 랜덤(50%)보다 통계적으로 유의미하게 높아야 함 |

자세한 안전장치 설명은 [SAFETY_MECHANISMS_KR.md](SAFETY_MECHANISMS_KR.md)를 참조하세요.

## 10. 할인율: γ = 0.99

- **의미**: 미래 보상의 현재 가치 비율
- **0.99 선택 이유**: GPU 가격 결정은 장기적 관점이 중요. 5일 후 보상도 현재의 95.1% 가치로 반영
- **0.9 vs 0.99 vs 0.999**:
  - 0.9: 1일 후 보상만 90% 반영, 너무 근시안적
  - 0.99: 5일 후 95.1%, 30일 후 74.0% — 적절한 장기 관점
  - 0.999: 30일 후 97.0% — 장기 보상을 과대평가

---

**참고 문헌**:
- Silver et al., "Mastering the game of Go without human knowledge" (AlphaGo Zero, 2017)
- Schrittwieser et al., "Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model" (MuZero, 2020)
- Auer et al., "Finite-time Analysis of the Multiarmed Bandit Problem" (UCB1, 2002)
