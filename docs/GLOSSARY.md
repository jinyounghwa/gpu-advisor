# Glossary

AI/ML terms used in the GPU Advisor project.

## A

### Action Space
The set of all possible actions the agent can take. In this project: {BUY_NOW, WAIT_SHORT, WAIT_LONG, HOLD, SKIP} — 5 discrete actions.

### AdamW
A variant of the Adam optimizer that decouples weight decay from gradient updates. Improves generalization by handling regularization independently from learning rate adaptation.

### Anti-Collapse Regularizer
A mechanism that prevents the policy from over-concentrating on a single action (mode collapse). When entropy drops below a threshold, it gently blends the policy toward the prior distribution.

## B

### Backup (MCTS)
The phase in MCTS where simulation results are propagated from the leaf node back to the root along the traversed path. Updates each node's visit count (N) and value sum (W).

### Backtest
Evaluation method using historical data. Tests whether the agent's decision at day t was correct given the actual price movement at day t+1.

### Batch Size
Number of samples used per training step. Set to 32 in this project.

## C

### Calibration
The process of blending the MCTS policy with other signals (reward, prior, utility) to produce the final action probabilities.

### Checkpoint
A saved file containing trained model weights (`.pth`). Includes state_dicts for all three networks (h, g, f) plus metadata.

### Confidence
The highest probability in the calibrated policy distribution. Range 0.0–1.0. Below 0.25 triggers safe mode.

### Cross-Entropy Loss
Standard loss function for classification. Measures the difference between predicted probability distribution and actual labels. Used for policy learning.

## D

### Dirichlet Noise
Exploration noise added to MCTS root node priors. With α = 0.03, adds small randomness to encourage diverse action exploration.

### Discount Factor (γ)
Present value ratio for future rewards. γ = 0.99 means a reward one step ahead is worth 99% of its face value. Influences long-term planning.

### Dynamics Network (g)
Part of the world model. Given current state s_t and action a_t, predicts the next state s_{t+1} and reward.

## E

### Entropy
A measure of uncertainty in a probability distribution. H = -Σ p(x) × log(p(x)). High entropy = uniform = uncertain. Low entropy = concentrated = confident.

### Expansion (MCTS)
The phase in MCTS where child nodes are added to a leaf node. The Prediction Network provides prior probabilities.

## F

### Feature Engineering
Transforming raw data into a format suitable for model learning. In this project: 11D raw → 256D feature vector.

### Fine-Tuning
Additional training of a pre-trained model on new data. Daily collected data continuously improves the world model.

## G

### GELU (Gaussian Error Linear Unit)
An activation function. A smooth variant of ReLU that allows small gradients for negative inputs. Common in Transformers.

### Gradient Clipping
Scaling gradients when their norm exceeds a threshold (1.0) to ensure training stability.

## H

### HOLD
One of the 5 actions. Neither buying nor avoiding — simply observing the market. Default action in safe mode.

## K

### KL Divergence (Kullback-Leibler Divergence)
A measure of difference between two probability distributions. Used during training to regularize the policy against drifting too far from the prior.

## L

### Latent State
The 256-dimensional vector generated by the Representation Network. A compressed representation of raw market data. Common input for other networks.

### LayerNorm (Layer Normalization)
Normalizes features within each sample independently. Stable with small batch sizes unlike BatchNorm.

### Learning Rate
The step size for weight updates. Set to 1e-4. Too large → divergence; too small → slow convergence.

## M

### MCTS (Monte Carlo Tree Search)
A search algorithm for decision-making. Repeats four phases — Selection → Expansion → Simulation → Backup — to find optimal actions. Core algorithm of AlphaGo.

### Mode Collapse
When a model over-concentrates on one output instead of diverse outputs. Defined as one action receiving ≥95% probability.

### Moving Average (MA)
Average price over the last N days. MA7 = 7-day moving average, MA14 = 14-day. Used to identify price trends.

### MSE Loss (Mean Squared Error)
Average of squared differences between predicted and actual values. Used for value prediction and reward prediction training.

### MuZero
DeepMind's algorithm extending AlphaZero. Plans using a learned world model (Dynamics Network) without knowing the environment's rules.

## N

### Node (MCTS)
Each position in the MCTS tree. Contains state, visit count (N), value sum (W), prior probability (P), and child node list.

## O

### One-Hot Encoding
Converting categorical values to binary vectors. Action 2 (WAIT_LONG) → [0, 0, 1, 0, 0].

## P

### Policy (π)
A mapping from states to actions. π(a|s) = probability of selecting action a in state s. Output of the Prediction Network.

### Positional Encoding
Adding position information to vectors using sin/cos functions. Gives each position a unique pattern in the sequence.

### Prediction Network (f)
Outputs policy (action probabilities) and value from the latent state. Corresponds to AlphaGo's Policy-Value Network.

### Prior Probability
The action distribution from training data. Reflects how often each action was optimal in historical data.

## Q

### Q-Value
Expected value of taking a specific action in a specific state. Q = W/N (total value / visit count).

### Quality Gates
Performance criteria that must be passed before agent deployment. 7 gates including accuracy, reward, abstain ratio, and entropy.

## R

### Representation Network (h)
Encodes market state (22D) into latent state (256D). The entry point of the world model.

### Reward
Numerical feedback for an action's outcome. BUY_NOW reward = actual price change percentage. Positive = good decision, negative = bad.

### Rollout
Simulating multiple steps ahead from a current MCTS node using the Dynamics Network to predict future states.

### RSI (Relative Strength Index)
A technical indicator expressing price momentum on a 0–100 scale. Above 70 = overbought, below 30 = oversold.

## S

### Safe Mode
Activated when agent confidence is low or entropy is high. Forces all actions to HOLD.

### Selection (MCTS)
The phase in MCTS where the tree is traversed from root to leaf by following the child with the highest UCB score.

### Softmax
Converts logits (real numbers) into a probability distribution (0–1, summing to 1). softmax(x_i) = exp(x_i) / Σ exp(x_j).

### State Vector
A numerical representation of market conditions at a specific time. Feature Engineer generates 256-dimensional vectors.

## T

### Tanh (Hyperbolic Tangent)
An activation function that bounds output to [-1, 1]. Used in the value head to normalize value estimates.

### Temperature (τ)
Controls exploration level when converting MCTS visit counts to action probabilities. τ=1: proportional to visits; τ=0: greedy (most-visited only).

### Transition Sample
A 5-tuple of (state, next_state, action, reward, value_target). The fundamental data unit for world model training.

## U

### UCB (Upper Confidence Bound)
A score balancing exploration and exploitation. UCB = Q (exploitation) + exploration bonus. Gives higher bonus to less-visited actions to encourage exploration.

### Uplift
Measures how much the agent outperforms baseline strategies (always buy, always wait, etc.).

## V

### Value (v)
Long-term expected return from a state. v(s) = expected cumulative reward when starting from state s and following the optimal policy. Range [-1, 1].

## W

### Weight Decay
A regularization technique adding a penalty proportional to weight magnitudes during training. Prevents overfitting. Also known as L2 regularization.

### World Model
A learned model that simulates the environment's dynamics. In this project: Representation (h) + Dynamics (g) + Prediction (f) — three networks.

---

**Note**: This glossary covers terms as used in this project's code and documentation.
